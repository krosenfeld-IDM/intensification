""" neighborhood_sir.py

The posterior class for the regularized SIR model adjusted for 
the incorporation of a spatial neighborhood to influence seasonality."""
import os

## Standard stuff
import numpy as np
import pandas as pd

## For matrix constructions
from scipy.sparse import diags

## For optimization
from scipy.optimize import minimize

class NeighborhoodPosterior(object):

	""" Limited version of the strong prior model to estimate SIA stuff only, which we need to
	create regularizing inputs for the state level """

	def __init__(self,df,sia_effects,S0,S0_var,beta_corr=3.,tau=26,mu_guess=0.1):

		## Store some information about the model
		self.T = len(df)-1
		self.beta_corr = beta_corr
		self.C_t = df["cases"].values
		self.B_t = df["adj_births"].values
		self.sias = sia_effects.values
		self.num_sias = self.sias.shape[1]

		## Regularize S0?
		self.logS0_prior = np.log(S0)
		self.logS0_prior_var = S0_var/(S0**2)
		self.logS0 = np.log(S0)
		self.logS0_var = S0_var/(S0**2)

		## Set the initial guess for mu
		self.mu = mu_guess*np.ones((self.num_sias,))
		self.mu_var = np.zeros((self.num_sias,))
		
		## Initialize key pieces, like the periodic smoothing
		## matrix.
		self.tau = tau
		D2 = np.diag(tau*[-2])+np.diag((tau-1)*[1],k=1)+np.diag((tau-1)*[1],k=-1)
		D2[0,-1] = 1 ## Periodic BCs
		D2[-1,0] = 1
		self.pRW2 = np.dot(D2.T,D2)*((beta_corr**4)/4.)

		## The design matrices for the linear portion of the 
		## transmission regression problem.
		self.X = np.vstack((int(len(df)-1/len(self.pRW2))+1)*[np.eye(len(self.pRW2))])[:len(df)-1]
		self.C = np.linalg.inv(np.dot(self.X.T,self.X)+self.pRW2)
		self.H = np.dot(self.X,np.dot(self.C,self.X.T))

		## And the adjusted case trace, which is the sum of all
		## the underlying state level traces.
		self.adj_cases = df["adj_cases_p"].values

	def compartment_df(self):

		## Compute key quantities
		adj_sias = (self.mu*self.sias[:-1]).sum(axis=1)
		E_t = self.adj_cases[1:]
		I_t = self.adj_cases[:-1]
		S_t = np.exp(self.logS0) + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		
		## Pack them up
		time = np.arange(0,len(I_t)+1)
		E_t = pd.Series(E_t,index=time[1:],name="E_t")
		I_t = pd.Series(I_t,index=time[:-1],name="I_t")
		S_t = pd.Series(S_t,index=time[:-1],name="S_t")
		out = pd.concat([E_t,I_t,S_t],axis=1)
		return out
		
	def fixed_rt(self,theta):

		## Unpack 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = self.adj_cases[1:]
		I_t = self.adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Solve the linear regression problem
		Y_hat = np.dot(self.H,Y_t)
		RSS = np.sum((Y_t-Y_hat)**2)
		lnp_beta = 0.5*self.T*np.log(RSS/self.T)

		## If you added S0 var
		lnp_S0 = 0.5*((theta[0]-self.logS0_prior)**2)/(self.logS0_prior_var)
		
		return lnp_beta + lnp_S0

	def fixed_rt_grad(self,theta):

		## Unpack 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = self.adj_cases[1:]
		I_t = self.adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Solve the linear regression problem and compute
		## The implied variance
		Y_hat = np.dot(self.H,Y_t)
		resid = Y_t-Y_hat
		var = np.sum(resid**2)/self.T

		## Compute the contribution from S_t
		dYdt0 = -S0/S_t 
		grad_t = np.dot(dYdt0-np.dot(self.H,dYdt0),resid)/var
		grad_t += (theta[0]-self.logS0_prior)/self.logS0_prior_var

		## Compute the contribution for mu
		dYdmu = np.cumsum(self.sias[:-1],axis=0)/S_t[:,np.newaxis]
		grad_mu = np.dot((dYdmu-np.dot(self.H,dYdmu)).T,resid)/var

		## Combine and return
		jac = np.zeros((len(theta),))
		jac[0] = grad_t
		jac[1:] = grad_mu
		return jac

	def fixed_rt_hessian(self,theta):

		## Unpack the S0-mu point of interest 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = self.adj_cases[1:]
		I_t = self.adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Solve the linear regression problem and compute
		## The implied variance
		Y_hat = np.dot(self.H,Y_t)
		resid = Y_t-Y_hat
		var = np.sum(resid**2)/self.T

		## Compute the S0-S0 term.
		dYdx0 = -S0/S_t 
		dYdx0dx0 = dYdx0**2 + dYdx0
		dS0dS0 = np.dot(dYdx0dx0-np.dot(self.H,dYdx0dx0),resid)/var+\
				 np.dot(dYdx0-np.dot(self.H,dYdx0),dYdx0-np.dot(self.H,dYdx0))/var+\
				 -(2./self.T)*((np.dot(dYdx0-np.dot(self.H,dYdx0),resid)/var)**2)
		dS0dS0 += 1./self.logS0_prior_var

		## Compute the mu-mu terms
		dYdmu = np.cumsum(self.sias[:-1],axis=0)/S_t[:,np.newaxis]
		dresid = dYdmu-np.dot(self.H,dYdmu)
		dmudmu = np.dot(dresid.T,dresid)/var+\
				 -(2./self.T)*np.dot(np.dot(dresid.T,resid),np.dot(dresid.T,resid).T)/var**2
		for i in range(self.num_sias):
			dYdmudi = dYdmu[:,i,np.newaxis]*dYdmu
			dmudmu[i,:] += np.dot((dYdmudi-np.dot(self.H,dYdmudi)).T,resid)/var
		
		## Compute the cross terms
		dYdx0dmu = dYdx0[:,np.newaxis]*dYdmu
		dS0dmu = np.dot((dYdx0dmu-np.dot(self.H,dYdx0dmu)).T,resid)/var+\
				 np.dot(dresid.T,dYdx0-np.dot(self.H,dYdx0))/var+\
				 -(2./self.T)*(np.dot(dresid.T,resid)/var)*(np.dot(dYdx0-np.dot(self.H,dYdx0),resid)/var)
		
		## Put it all together
		hess = np.zeros((self.num_sias+1,self.num_sias+1))
		hess[0,0] = dS0dS0
		hess[0,1:] = dS0dmu
		hess[1:,0] = dS0dmu
		hess[1:,1:] = dmudmu

		return hess

class HoodRegularizedModel(object):

	def __init__(self,df,sia_effects,S0,S0_var,hood_t,beta_corr=3.,tau=26,mu_guess=0.1):

		## Store some information about the model
		self.T = len(df)-1
		self.beta_corr = beta_corr
		self.C_t = df["cases"].values
		self.B_t = df["adj_births"].values
		self.sias = sia_effects.values
		self.num_sias = self.sias.shape[1]

		## Regularize S0?
		self.logS0_prior = np.log(S0)
		self.logS0_prior_var = S0_var/(S0**2)
		self.logS0 = np.log(S0)
		self.logS0_var = S0_var/(S0**2)

		## Set the initial guess for mu
		self.mu = mu_guess*np.ones((self.num_sias,))
		self.mu_var = np.zeros((self.num_sias,))

		## And the reporting rate prior pieces
		self.r_prior = df["rr_p"].values
		self.r_prior_precision = np.diag(1./(df["rr_p_var"].values))
		self.r_floor = (df["rr_p"]-4.*np.sqrt(df["rr_p_var"])).min()
		self.r_hat = df["rr_p"].values
		
		## Initialize key pieces, like the periodic smoothing
		## matrix.
		self.tau = tau
		D2 = np.diag(tau*[-2])+np.diag((tau-1)*[1],k=1)+np.diag((tau-1)*[1],k=-1)
		D2[0,-1] = 1 ## Periodic BCs
		D2[-1,0] = 1
		self.pRW2 = np.dot(D2.T,D2)*((beta_corr**4)/4.)

		## Which fits into a regularization matrix sized to account
		## for the scale factor between seasonalities (frequency dependent 
		## transmission, lol)
		self.lam = np.zeros((self.tau+1,self.tau+1))
		self.lam[1:,1:] = self.pRW2

		## And additionally the covariate associated with the neighborhood
		## dynamics.
		self.Y_N = np.log(hood_t["E_t"].values[1:])-\
					np.log(hood_t["I_t"].values[:-1])-\
					np.log(hood_t["S_t"].values[:-1])
		assert len(self.Y_N) == self.T,\
			"The state and hood dfs have to be on the same time-scale."

		## Construct the full design matrix, first by making one for a single
		## time series, then duplicating it and adding the scale factor column.
		X_t = np.vstack((int(len(df)-1/self.tau)+1)*[np.eye(self.tau)])[:len(df)-1]
		self.X = np.vstack([X_t,X_t])
		self.X = np.hstack([np.zeros((len(self.X),1)),
							self.X])
		self.X[:self.T,0] = 1.

		## The design matrices for the linear portion of the 
		## transmission regression problem.
		self.C = np.linalg.inv(np.dot(self.X.T,self.X)+self.lam)
		self.H = np.dot(self.X,np.dot(self.C,self.X.T))

	def fixed_mu(self,theta):

		## Unpack the input
		S0 = np.exp(theta[0])
		r_t = theta[1:]
	
		## Compute the implied model compartments
		adj_cases = (self.C_t+1.)/r_t - 1.
		adj_sias = (self.mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])

		## Solve the linear regression problem
		Y_hat = np.dot(self.H,Y_tot)
		RSS = np.sum((Y_tot-Y_hat)**2)
		lnp_beta = 0.5*(2.*self.T)*np.log(RSS/(2.*self.T))

		## If you added S0 var
		lnp_S0 = 0.5*((theta[0]-self.logS0_prior)**2)/self.logS0_prior_var
		
		## Compute the r_t component
		lnp_rt = 0.5*np.sum(((r_t-self.r_prior)**2)*np.diag(self.r_prior_precision))

		return lnp_beta + lnp_rt + lnp_S0

	def fixed_mu_grad(self,theta):

		## Unpack the input
		S0 = np.exp(theta[0])
		r_t = theta[1:]

		## Compute the implied model compartments
		adj_cases = (self.C_t+1.)/r_t - 1.
		adj_sias = (self.mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])

		## Solve the linear regression problem and compute
		## The implied variance
		Y_hat = np.dot(self.H,Y_tot)
		resid = Y_tot-Y_hat
		var = np.sum(resid**2)/(2.*self.T)

		## Compute the contribution from S_t
		dYdt0 = -S0/S_t 
		dYdt0 = np.hstack([dYdt0,np.zeros(dYdt0.shape)])
		grad_t = np.dot(dYdt0-np.dot(self.H,dYdt0),resid)/var
		grad_t += (theta[0]-self.logS0_prior)/self.logS0_prior_var

		## Compute the contribution for r_t 
		dYtdrt = (I_t+1.)/(I_t*r_t[:-1])
		dYtdrt_plus1 = -(E_t+1.)/(E_t*r_t[1:])
		dYdr = diags([dYtdrt,dYtdrt_plus1],[0,1],shape=(self.T,self.T+1)).todense()
		dYdr[:,1:] += -np.tril(np.outer(1./S_t,(E_t+1.)/r_t[1:]))
		dYdr = np.vstack([dYdr,np.zeros(dYdr.shape)])
		grad_r = np.dot((dYdr-np.dot(self.H,dYdr)).T,resid)/var
		grad_r += np.dot(self.r_prior_precision,r_t-self.r_prior)
		
		## Combine and return
		jac = np.zeros((len(theta),))
		jac[0] = grad_t
		jac[1:] = grad_r
		return jac

	def fixed_rt(self,theta):

		## Unpack 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_cases = (self.C_t+1.)/self.r_hat - 1.
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])
		
		## Solve the linear regression problem
		Y_hat = np.dot(self.H,Y_tot)
		RSS = np.sum((Y_tot-Y_hat)**2)
		lnp_beta = 0.5*(2.*self.T)*np.log(RSS/(2.*self.T))

		## If you added S0 var
		lnp_S0 = 0.5*((theta[0]-self.logS0_prior)**2)/(self.logS0_prior_var)
		
		return lnp_beta + lnp_S0

	def fixed_rt_grad(self,theta):

		## Unpack 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_cases = (self.C_t+1.)/self.r_hat - 1.
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])
		
		## Solve the linear regression problem and compute
		## The implied variance
		Y_hat = np.dot(self.H,Y_tot)
		resid = Y_tot-Y_hat
		var = np.sum(resid**2)/(2.*self.T)

		## Compute the contribution from S_t
		dYdt0 = -S0/S_t
		dYdt0 = np.hstack([dYdt0,np.zeros(dYdt0.shape)])
		grad_t = np.dot(dYdt0-np.dot(self.H,dYdt0),resid)/var
		grad_t += (theta[0]-self.logS0_prior)/self.logS0_prior_var

		## Compute the contribution for mu
		dYdmu = np.cumsum(self.sias[:-1],axis=0)/S_t[:,np.newaxis]
		dYdmu = np.vstack([dYdmu,np.zeros(dYdmu.shape)])
		grad_mu = np.dot((dYdmu-np.dot(self.H,dYdmu)).T,resid)/var

		## Combine and return
		jac = np.zeros((len(theta),))
		jac[0] = grad_t
		jac[1:] = grad_mu
		return jac

	def fixed_rt_hessian(self,theta):

		## Unpack the S0-mu point of interest 
		S0 = np.exp(theta[0])
		mu = theta[1:]

		## Compute key quantities
		adj_cases = (self.C_t+1.)/self.r_hat - 1.
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])

		## Solve the linear regression problem and compute
		## The implied variance
		Y_hat = np.dot(self.H,Y_tot)
		resid = Y_tot-Y_hat
		var = np.sum(resid**2)/(2.*self.T)

		## Compute the S0-S0 term.
		dYdx0 = -S0/S_t 
		dYdx0dx0 = dYdx0**2 + dYdx0
		dYdx0 = np.hstack([dYdx0,np.zeros(dYdx0.shape)])
		dYdx0dx0 = np.hstack([dYdx0dx0,np.zeros(dYdx0dx0.shape)])
		dS0dS0 = np.dot(dYdx0dx0-np.dot(self.H,dYdx0dx0),resid)/var+\
				 np.dot(dYdx0-np.dot(self.H,dYdx0),dYdx0-np.dot(self.H,dYdx0))/var+\
				 -(2./(2.*self.T))*((np.dot(dYdx0-np.dot(self.H,dYdx0),resid)/var)**2)
		dS0dS0 += 1./self.logS0_prior_var

		## Compute the mu-mu terms
		dYdmu = np.cumsum(self.sias[:-1],axis=0)/S_t[:,np.newaxis]
		dYdmu = np.vstack([dYdmu,np.zeros(dYdmu.shape)])
		dresid = dYdmu-np.dot(self.H,dYdmu)
		dmudmu = np.dot(dresid.T,dresid)/var+\
				 -(2./(2.*self.T))*np.dot(np.dot(dresid.T,resid),np.dot(dresid.T,resid).T)/var**2
		for i in range(self.num_sias):
			dYdmudi = dYdmu[:,i,np.newaxis]*dYdmu
			dmudmu[i,:] += np.dot((dYdmudi-np.dot(self.H,dYdmudi)).T,resid)/var
		
		## Compute the cross terms
		dYdx0dmu = dYdx0[:,np.newaxis]*dYdmu
		dS0dmu = np.dot((dYdx0dmu-np.dot(self.H,dYdx0dmu)).T,resid)/var+\
				 np.dot(dresid.T,dYdx0-np.dot(self.H,dYdx0))/var+\
				 -(2./(2.*self.T))*(np.dot(dresid.T,resid)/var)*(np.dot(dYdx0-np.dot(self.H,dYdx0),resid)/var)
		
		## Put it all together
		hess = np.zeros((self.num_sias+1,self.num_sias+1))
		hess[0,0] = dS0dS0
		hess[0,1:] = dS0dmu
		hess[1:,0] = dS0dmu
		hess[1:,1:] = dmudmu

		return hess

	def __call__(self,theta):

		## Unpack 
		S0 = np.exp(theta[0])
		mu = theta[1:self.num_sias+1]
		r_t = theta[1+self.num_sias:]

		## Compute key quantities
		adj_cases = (self.C_t+1.)/r_t - 1.
		adj_sias = (mu*self.sias[:-1]).sum(axis=1)
		E_t = adj_cases[1:]
		I_t = adj_cases[:-1]
		S_t = S0 + np.cumsum(self.B_t[:-1]-E_t-adj_sias)
		Y_t = np.log(E_t)-np.log(I_t)-np.log(S_t)

		## Augment Y_t with the neighboorhood information
		Y_tot = np.hstack([Y_t,self.Y_N])
		
		## Solve the linear regression problem
		Y_hat = np.dot(self.H,Y_tot)
		RSS = np.sum((Y_tot-Y_hat)**2)
		lnp_beta = 0.5*(2.*self.T)*np.log(RSS/(2.*self.T))

		## If you added S0 var
		lnp_S0 = 0.5*((theta[0]-self.logS0_prior)**2)/(self.logS0_prior_var)
		
		## Compute the r_t component
		lnp_rt = 0.5*np.sum(((r_t-self.r_prior)**2)*np.diag(self.r_prior_precision))

		return lnp_beta + lnp_rt + lnp_S0

## Some helper functions
def get_age_pyramid(state,fname=os.path.join("_data","grid3_population_by_state.csv")):

    ## Get the output from geopode
    df = pd.read_csv(fname,index_col=0)\
            .set_index(["state","age_bin"])
    df = df.loc[state].reset_index()
    population = int(np.round(df["total"].sum()))
    
    ## Make an age column, representing the start 
    ## of the age bins
    df["age"] = df["age_bin"].apply(lambda s: int(s.split()[0]))
    df = df.sort_values("age")

    ## Interpolate to a 
    pyramid = df[["age","total"]].set_index("age")["total"]
    pyramid = pyramid.reindex(np.arange(pyramid.index[-1]+5)).fillna(method="ffill")
    pyramid.loc[1:4] = pyramid.loc[1:4]/4
    pyramid.loc[5:] = pyramid.loc[5:]/5

    ## Turn it into a distribution
    pyramid = pyramid/population

    return pyramid, population

def prep_model_inputs(state,time_index,epi,cr,dists,mcv1_effic=0.825,mcv2_effic=0.95):

    ## Start by aggregating the epi data
    df = epi.resample("SMS").agg({"cases":"sum",
                                  "rejected":"sum",
                                  "births":"sum",
                                  "births_var":"sum",
                                  "mcv1":"mean",
                                  "mcv1_var":"mean",
                                  "mcv2":"mean",
                                  "mcv2_var":"mean"})
    df["births"] = df["births"].rolling(2).mean()
    df["births_var"] = df["births_var"].rolling(2).mean()
    df = df.loc[time_index] 

    ## Add a population column
    _, population = get_age_pyramid(state)
    df["population"] = len(df)*[population]

    ## Unpack the coarse regression outputs 
    ## and interpolate to the fine time scale.
    initial_S0 = cr.loc[2009,"S0"]
    initial_S0_var = cr.loc[2009,"S0_var"]
    rr_prior = cr[["rr","rr_var"]].copy().reset_index()
    rr_prior.columns = ["time","rr_p","rr_p_var"]
    rr_prior["time"] = pd.to_datetime({"year":rr_prior["time"],
                                       "month":1,
                                       "day":15})
    rr_prior = rr_prior.set_index("time")
    rr_prior = rr_prior.resample("d").interpolate().reindex(df.index)
    rr_prior = rr_prior.fillna(method="bfill").fillna(method="ffill")
    
    ## Add the reporting rate prior information to the 
    ## overall dataframe.
    df = pd.concat([df,rr_prior],axis=1)

    ## And the initial condition information
    df["initial_S0"] = len(df)*[initial_S0]
    df["initial_S0_var"] = len(df)*[initial_S0_var]

    ## Set up some survival corrections.
    survival = 1.-np.cumsum(0*dists,axis=1)
    pr_sus_at_mcv1 = survival[0]
    pr_sus_at_mcv1.index = pd.to_datetime({"year":pr_sus_at_mcv1.index,
                                           "month":6,"day":15})
    pr_sus_at_mcv1 = pr_sus_at_mcv1.resample("d").interpolate().reindex(time_index)
    pr_sus_at_mcv1 = pr_sus_at_mcv1.fillna(method="ffill")
    pr_sus_at_mcv2 = survival[1]
    pr_sus_at_mcv2.index = pd.to_datetime({"year":pr_sus_at_mcv2.index,
                                           "month":6,"day":15})
    pr_sus_at_mcv2 = pr_sus_at_mcv2.resample("d").interpolate().reindex(time_index)
    pr_sus_at_mcv2 = pr_sus_at_mcv2.fillna(method="ffill")

    ## Set up vaccination
    df["v1"] = (df["births"]*mcv1_effic*pr_sus_at_mcv1*df["mcv1"]).shift(18).fillna(method="bfill")
    df["v1_var"] = mcv1_effic*pr_sus_at_mcv1*(df["births"]*df["mcv1"]*(1.-df["mcv1"])+\
                    df["mcv1_var"]*(df["births"]**2)+\
                    df["births_var"]*(df["mcv1"]**2)).shift(18).fillna(method="bfill")

    ## Compute immunizations from MCV2
    mcv1_failures = df["v1"]*(1.-mcv1_effic)/mcv1_effic
    mcv1_failures_var = df["v1_var"]*(1.-mcv1_effic)/mcv1_effic
    df["v2"] = (mcv2_effic*df["mcv2"]*pr_sus_at_mcv2*mcv1_failures).shift(30-18).fillna(method="bfill")
    df["v2_var"] = mcv2_effic*pr_sus_at_mcv2*(mcv1_failures*df["mcv2"]*(1.-df["mcv2"])+\
                    df["mcv2_var"]*(mcv1_failures**2)+\
                    mcv1_failures_var*(df["mcv2"]**2)).shift(30-18).fillna(method="bfill")

    ## Construct adjusted births
    df["adj_births"] = df["births"]-df["v1"]-df["v2"]
    df["adj_births_var"] = df["births_var"]+df["v1_var"]+df["v2_var"]

    ## Collect effects besides SIA and initial susceptibility
    df = df.loc["2009-01-01":]
    df["S_t_tilde"] = np.cumsum(df["adj_births"])

    ## And finally compute prior adjusted cases
    df["adj_cases_p"] = (df["cases"]+1.)/df["rr_p"] - 1.

    return df

def prep_sia_effects(cal,time_index):

    ## Get the SIA calendar to collect SIA effects, looping over campaigns
    ## and aligning to the time steps 
    cal = cal.loc[(cal["start_date"] >= time_index[0]) &\
                  (cal["start_date"] <= time_index[-1])]
    cal = cal.sort_values("start_date").reset_index(drop=True)
    cal["time"] = cal["start_date"]+0.5*(cal["end_date"].fillna(cal["start_date"])-cal["start_date"])
    cal["time"] = cal["time"].dt.round("d")
    sia_effects = cal[["time","doses"]].copy()
    sia_effects["time"] = sia_effects["time"].apply(lambda t: np.argmin(np.abs(t-time_index)))
    sia_effects["time"] = time_index[sia_effects["time"].values]
    
    ## Consolidate any overlapping dates, and reshape into one
    ## timeseries per SIA, with the doses at the approporate dates
    sia_effects = sia_effects.groupby("time").sum().reset_index()
    sia_effects = sia_effects.reset_index().rename(columns={"index":"sia_num"})
    sia_effects = sia_effects.pivot(index="time",columns="sia_num",values="doses")
    sia_effects = sia_effects.reindex(time_index).fillna(0)

    return sia_effects

def fit_the_neighborhood_model(region,hood_df,hood_sias,initial_mu_guess=0.1):

	## Create a model object.
    hoodP = NeighborhoodPosterior(
                hood_df,
                hood_sias,
                hood_df["initial_S0"].values[0],
                hood_df["initial_S0_var"].values[0],
                beta_corr=3.,
                tau=24,
                mu_guess=initial_mu_guess,
                )

    ## Fit this auxillary model by finding good SIAS given the
    ## coarse regression approximation to r_t
    x0 = np.ones((hoodP.num_sias+1,))
    x0[0] = hoodP.logS0_prior
    x0[1:] = hoodP.mu
    sia_op = minimize(hoodP.fixed_rt,
                      x0=x0,
                      jac=hoodP.fixed_rt_grad,
                      method="L-BFGS-B",
                      bounds=[(None,None)]+(len(x0)-1)*[(0,1)],
                      options={"ftol":1e-13,
                               "maxcor":100,
                               },
                      )
    print("\nResult from SIA optimization for the {}"
          " region... ".format(region.title()))
    print("Success = {}".format(sia_op.success))
    hoodP.logS0 = sia_op["x"][0]
    hoodP.mu = sia_op["x"][1:]

    return hoodP

def fit_the_regularized_model(state,state_df,state_sias,hood_t,initial_mu_guess=0.5):

	## Then use that estimate of the compartments to
    ## inform seasonality in the state level model.
    neglp = HoodRegularizedModel(
    			state_df,
                state_sias,
                state_df["initial_S0"].values[0],
                state_df["initial_S0_var"].values[0],
                hood_t,
                beta_corr=3.,
                tau=24,
                mu_guess=initial_mu_guess)

    ## Fit the model by first finding good SIAS given the
    ## coarse regression approximation to r_t
    x0 = np.ones((neglp.num_sias+1,))
    x0[0] = neglp.logS0_prior
    x0[1:] = neglp.mu
    sia_op = minimize(neglp.fixed_rt,
                      x0=x0,
                      jac=neglp.fixed_rt_grad,
                      method="L-BFGS-B",
                      bounds=[(None,None)]+(len(x0)-1)*[(0,1)],
                      options={"ftol":1e-13,
                               "maxcor":100,
                               },
                      )
    print("\nResult from fixed r_t SIA optimization for just {}:".format(state))
    print("Success = {}".format(sia_op.success))
    neglp.logS0 = sia_op["x"][0]
    neglp.mu = sia_op["x"][1:]

    ## Then adjust the reporting rate given the SIAs
    x0 = np.ones((1+neglp.T+1,))
    x0[0] = neglp.logS0
    x0[1:] = neglp.r_hat
    rep_op = minimize(neglp.fixed_mu,
                      x0=x0,
                      jac=neglp.fixed_mu_grad,
                      method="L-BFGS-B",
                      bounds=[(None,None)]+(len(x0)-1)*[(5.e-4,1)],
                      options={"ftol":1e-13,
                               "maxcor":100,
                               },
                      )
    print("\n...And from fixed SIA r_t optimization")
    print("Success = {}".format(rep_op.success))
    neglp.logS0 = rep_op["x"][0]
    neglp.r_hat = rep_op["x"][1:]

    ## Compute the covariance matrix conditional on the
    ## reporting adjusted estimates
    x0 = np.ones((neglp.num_sias+1,))
    x0[0] = neglp.logS0
    x0[1:] = neglp.mu
    hessian = neglp.fixed_rt_hessian(x0)
    cov = np.linalg.inv(hessian)
    
    ## Finalize the uncertainty estimates
    overall_var = np.diag(cov)
    neglp.logS0_var = overall_var[0]
    neglp.mu_var = overall_var[1:]  

    ## Finally, fully specify the transmission parameters using the
    ## SIA and reporting rate estimates via the optimization above
    adj_cases = ((state_df["cases"].values+1.)/neglp.r_hat)-1.
    adj_births = state_df["adj_births"].values
    adj_sias = (neglp.mu*neglp.sias[:-1]).sum(axis=1)
    E_t = adj_cases[1:]
    I_t = adj_cases[:-1]
    S_t = np.exp(neglp.logS0)+np.cumsum(state_df["adj_births"].values[:-1]-E_t-adj_sias)

    ## Which let's us estimate via a single log-linear regression 
    ## the alpha != 1 model parameters...
    print("\nSpecifying the final transmission term...")
    Y_t = np.log(E_t)-np.log(S_t)
    X = np.hstack([neglp.X[:neglp.T,1:],np.log(I_t)[:,np.newaxis]])
    pRW2 = np.zeros((X.shape[1],X.shape[1]))
    pRW2[:-1,:-1] = neglp.pRW2
    C = np.linalg.inv(np.dot(X.T,X)+pRW2)
    beta_hat = np.dot(C,np.dot(X.T,Y_t))
    beta_t = np.dot(X,beta_hat)
    RSS = np.sum((Y_t-beta_t)**2)
    sig_eps = np.sqrt(RSS/(neglp.T))#-X.shape[1]))
    print("sig_eps = {}".format(sig_eps))
    beta_cov = sig_eps*sig_eps*C
    beta_var = np.diag(beta_cov)
    beta_std = np.sqrt(beta_var)
    beta_t_std = np.sqrt(np.diag(np.dot(X,np.dot(beta_cov,X.T))))
    inf_seasonality = np.exp(beta_hat[:-1])
    inf_seasonality_std = np.exp(beta_hat[:-1])*beta_std[:-1]
    alpha = beta_hat[-1]
    alpha_std = beta_std[-1]
    print("alpha = {} +/- {}".format(alpha,2.*alpha_std))

    return neglp, inf_seasonality, inf_seasonality_std, alpha, sig_eps